{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "603cb713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb64c34",
   "metadata": {},
   "source": [
    "### Q.1) Write python code to implement a neural network with one hidden layer forclassifying an XOR gate. Implement the backpropagation algorithm for this case from scratch. Use two neurons for the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59d9f9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4)\n",
      "[[1 1 0 0]\n",
      " [1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "X=np.array([[1,1],[1,0],[0,1],[0,0]])\n",
    "x_train=X.T\n",
    "print(x_train.shape)\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91af68da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4)\n",
      "[[0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "Y=np.array([0,1,1,0]).reshape(4,1)\n",
    "y_train=Y.T\n",
    "print(y_train.shape)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76464171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84223e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_relu(z):\n",
    "    return 1*(z>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ba49e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_function(z):\n",
    "    return 1/(1+np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e66f7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_sigmoid(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f4284cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y,ycap):\n",
    "    m=y.shape[1]\n",
    "    cost=-(1/m)*np.sum(y*np.log(ycap))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "589698a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_initialization(layer_attributes):\n",
    "    \n",
    "    L=len(layer_attributes)-1\n",
    "    W=[]\n",
    "    B=[]\n",
    "    for i in range(1,L+1):\n",
    "        weight_i = np.random.randn(layer_attributes[i],layer_attributes[i-1])\n",
    "        bias_i=np.zeros((layer_attributes[i],1))\n",
    "        W.append(weight_i)\n",
    "        B.append(bias_i)\n",
    "    return W,B\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a91bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FORWARD PROPOGATION\n",
    "def forward_propagation(x,w,b):\n",
    "\n",
    "    A=[]\n",
    "    Z=[]\n",
    "    length=len(w)\n",
    "    A.append(x)\n",
    "    #hidden layer\n",
    "    for i in range(length-1):\n",
    "        z_i=np.dot(w[i],A[-1])+b[i]\n",
    "        Z.append(z_i)\n",
    "        a_i = relu(z_i)\n",
    "        A.append(a_i)\n",
    "    #output layer\n",
    "    z_l = np.dot(w[-1],A[-1]) + b[-1]\n",
    "    a_l = sigmoid_function(z_l)\n",
    "    A.append(a_l)\n",
    "    Z.append(z_l)\n",
    "\n",
    "    return Z,A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d31de394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BACK PROPOGATION\n",
    "def back_prop(A,y,W,B,Z):\n",
    "    m=y.shape[1]\n",
    "    L=len(W)\n",
    "    dW=[]\n",
    "    dB=[]\n",
    "    dZ=[]\n",
    "    #Output Layer\n",
    "    dZ.append((A[-1]-y))\n",
    "    dB.append((1/m)*np.sum(dZ[-1],axis=1,keepdims=True))\n",
    "    dW.append((1/m)*(np.dot(dZ[-1],A[-2].T)))\n",
    "   \n",
    "    #Hidden layers\n",
    "    l=L-1\n",
    "    while l >0:\n",
    "        dz_l = (1/m)*np.dot(W[l].T,dZ[-1])*derivative_relu(A[l])\n",
    "        db_l = (1/m)*np.sum(dz_l,axis=1,keepdims=True)\n",
    "        dw_l = (1/m)*np.dot(dz_l,A[l-1].T)\n",
    "        dW.append(dw_l)\n",
    "        dB.append(db_l)\n",
    "        dZ.append(dz_l)\n",
    "        l=l-1\n",
    "            \n",
    "    return dZ[::-1],dW[::-1],dB[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c56c3f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(W,B,dW,dB,learning_rate):\n",
    "    \n",
    "    alpha=learning_rate\n",
    "    length=len(W)\n",
    "    for i in range(length):\n",
    "        W[i] = W[i] - alpha*dW[i]\n",
    "        B[i] = B[i] - alpha*dB[i]\n",
    "    return W,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37294df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(x_train,y_train,learning_rate = 0.1,epochs = 2000,num_hidden_layers = 1,neurons=3):\n",
    "\n",
    "    layer=[]\n",
    "    n,m=x_train.shape\n",
    "    J_train=[]\n",
    "    layer.append(x_train.shape[0])\n",
    "    for i in range(num_hidden_layers):\n",
    "        layer.append(neurons)\n",
    "    layer.append(y_train.shape[0])\n",
    "    print(f'neuron configuration: {layer}')\n",
    "    w,b=random_initialization(layer)\n",
    "    for j in range(epochs):\n",
    "        z,a = forward_propagation(x_train,w,b)\n",
    "        dz,dw,db=back_prop(a,y_train,w,b,z)\n",
    "        w,b=gradient_descent(w,b,dw,db,learning_rate)\n",
    "        cost_train=binary_cross_entropy(y_train,a[-1])\n",
    "        J_train.append(cost_train)\n",
    "        if j%(epochs/10)==0:\n",
    "            print(f' \\n epoch:{j:4d}  Train error: {J_train[-1]:8.2f} ')\n",
    "            \n",
    "    return w,b,a  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b20d4058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuron configuration: [2, 2, 1]\n",
      " \n",
      " epoch:   0  Train error:     0.06 \n",
      " \n",
      " epoch:1000  Train error:     0.23 \n",
      " \n",
      " epoch:2000  Train error:     0.03 \n",
      " \n",
      " epoch:3000  Train error:     0.01 \n",
      " \n",
      " epoch:4000  Train error:     0.01 \n",
      " \n",
      " epoch:5000  Train error:     0.00 \n",
      " \n",
      " epoch:6000  Train error:     0.00 \n",
      " \n",
      " epoch:7000  Train error:     0.00 \n",
      " \n",
      " epoch:8000  Train error:     0.00 \n",
      " \n",
      " epoch:9000  Train error:     0.00 \n"
     ]
    }
   ],
   "source": [
    "w,b,a=neural_network(x_train,y_train,neurons=2,epochs=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c77d0674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00169547 0.99758522 0.99758524 0.00836327]]\n",
      "[0. 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "z,a=forward_propagation(x_train,w,b)\n",
    "print(a[-1])\n",
    "y_pred_actual=a[-1]\n",
    "b=np.zeros((a[-1].shape[1]))\n",
    "for i in range(b.shape[0]):\n",
    "    if a[-1][:,i]>=0.5:\n",
    "        b[i]=1\n",
    "    elif a[-1][:,i]<0.5:\n",
    "        b[i]=0\n",
    "print(b)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "928a6099",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=np.squeeze(y_train)\n",
    "y_pred_actual=np.squeeze(y_pred_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37fe88e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>Y_TRUE</th>\n",
       "      <th>SIGMOID OUTPUT</th>\n",
       "      <th>Y_PRED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997585</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997585</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008363</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X1  X2  Y_TRUE  SIGMOID OUTPUT  Y_PRED\n",
       "0   1   1       0        0.001695     0.0\n",
       "1   1   0       1        0.997585     1.0\n",
       "2   0   1       1        0.997585     1.0\n",
       "3   0   0       0        0.008363     0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''TRUTH TABLE'''\n",
    "p=X[:,0]\n",
    "q=X[:,1]\n",
    "outputdf = pd.DataFrame({'X1':p,'X2':q,'Y_TRUE':y_train,'SIGMOID OUTPUT':y_pred_actual,'Y_PRED':b})\n",
    "outputdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42cd14c",
   "metadata": {},
   "source": [
    "### INFERENCES\n",
    "The neural network to find the Truth table for XOR gate has been implemented from scratch without the use of any automatic-differentiation packages such as Tensorflow or PyTorch.\n",
    "The activation function used in the hidden layer is ReLu function and in the output layer is Sigmoid function.\n",
    "#### NOTE: Since the data points are very less(4) and the given problem is a classification problem, it was found out that the output depends heavily on the weight initializations. So, using np.random.randn() function, there are chances that in some initializations, the neural network wont converge to required results which can be seen from the cost value as it will be high and constant with increasing epochs. To rectify this, the weights should be initialized in such a way that the relu function in the 2 neurons of the input layer can form a non-linear decision boundary which seperates the given 4 datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36886dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
